# doutorado
Projeto utilizado na coleta e análise dos dados de uma pesquisa de doutorado.

SOBRE A PESQUISA:
Os notebooks acoplamento(resumo/palavras-chaves).ipynb, coleta_metadados.ipynb, analise_descritiva.ipynb, mineracao_regras_associacao.ipynb e resumos_artigos.ipynb foram escritos para a coleta e análise de dados da pesquisa de doutorado intitulada “Sujeito Coletivo, lutas LGBTQIA+ e a esquerda contemporânea: Uma Análise de Conteúdo da noção de Sujeito Universal Contingente nos estudos sobre os movimentos sociais LGBTQIA+”. A pesquisa em questão buscou mapear a produção acadêmica sobre os movimentos sociais LGBTQIA+ no Brasil, com o intuito de compreender como os estudiosos têm abordado a construção do “sujeito coletivo”. Metodologicamente, a pesquisa é bibliográfica e documental, analisando publicações sobre movimentos LGBTQIA+ no Brasil entre 2000 e 2022. Utiliza a Análise de Conteúdo Categorial para examinar como o conceito de Sujeito Coletivo é mobilizado nos estudos selecionados. Ferramentas de Machine Learning auxiliaram na coleta e na análise dos dados, ajudando a identificar padrões relevantes. Os dados revelam uma diversidade de instituições com publicações de artigos sobre o movimento e predominância das áreas de Psicologia, Sociologia e Antropologia. 
O problema central a que esta pesquisa busca responder é: “Como a noção de um Sujeito Coletivo Universal Contingente tem sido mobilizado nos estudos sobre o movimento LGBTQIA+ brasileiro?” Entre seus principais objetivos estão 1) mapear as pesquisas empíricas sobre os movimentos sociais LGBTQIA+ no Brasil; e 2) analisar como o Sujeito Coletivo do movimento é entendido nestes estudos. 

PROCEDIMENTOS DE PESQUISA:
Uma vez em posse dos 96 artigos em formato PDF, eles foram convertidos em arquivos de Word usando o Google Drive e resumidos através da biblioteca Python Sumy. Como esta pesquisa é uma Análise de Conteúdo indutiva, não trabalhamos com categorias previamente construídas, optamos por construí-las ao longo da leitura dos dados. As sentenças atribuídas a cada código foram lidas e analisadas. De acordo com as ideias expressas no texto, atribuímos a cada sentença — e/ou conjunto de sentenças que expressam a mesma ideia — uma ou mais categorias para descrever aquela ideia criada no momento da leitura dos textos em análise ou analisados anteriormente. Ao total, emergiram 211 categorias no processo. Cada categoria foi representada por uma variável (coluna) em um banco de dados em .csv e quando o artigo apresentava sentenças que se enquadra em uma dada categoria, a variável referente a categoria recebeu um valor 1, se não havia sentenças no artigo que se enquadra na categoria, recebeu o valor 0. Cada uma das 211 categorias foi adicionada ao livro de códigos bem como uma descrição do significado de cada uma. Foi utilizado o Google Colab para compilar os códigos em Python.

COMO OS NOTEBOOKS FORAM USADOS NA PESQUISA:
Nossa proposta inicial para esta pesquisa seria realizar um estudo cientométrico com os artigos sobre os movimentos sociais LGBTQIA+. Nos questionamos, então, quais metadados poderíamos extrair dos arquivos em PDF dos textos e quais ferramentas poderíamos utilizar para produzir tais dados. Após testar algumas ferramentas, decidimos que a maneira mais eficaz de coletar seria através do algoritmo de Processamento de Língua Natural que extrai Expressões Regulares. O notebook coleta_metadados.ipynb implentou o código responsável por executar a coleta de dados com Expressões Regulares.
Algoritmos de expressão regular são ferramentas utilizadas para identificar, extrair e manipular padrões específicos dentro de cadeias de texto. Eles funcionam ao definir uma sequência de caracteres que formam um padrão de busca, utilizando uma sintaxe especial que permite a combinação de caracteres literais e metacaracteres (como *, +, ?, [ ], { }, etc.). As expressões regulares são amplamente usadas em linguagens de programação e ferramentas de processamento de texto devido à sua flexibilidade e poder de descrição de padrões complexos de maneira concisa. Entretanto, como a formatação dos artigos é diferente de uma revista para a outra, a busca de informações como o título ou nome dos/as autores/as não poderia ser feita de forma padronizada sem antes um pré-processamento dos texto. Os artigos em PDF foram convertidos para o formato do word (.docx) através da ferramenta Google Drive. Nos documentos de word gerados desta forma, foram inseridos marcadores que permitiram ao algoritmo reconhecer os metadados de interesse.
Os metadados extraídos usando Expressões Regulares foram inseridos em dataframe. O notebook analise_descritiva.ipynb foi escrito para gerar tabelas de frequência e gráficos que forneceram as análises iniciais para a pesquisa. Com os dados dos resumos e palavras-chaves obtidos usando o notebook de coleta, conseguimos gerar dados sobre o acoplamento dos resumos e das palavras-chaves como proposto por Mira e Castanha (2023). Através do notebook acoplamento(resumo/palavras-chaves).ipynb, foi possível calcular um vetor numérico que representa o texto do resumo, calcular o cosseno de similaridade entre os pares de resumos, as correlações de Pearson e Spearman e, através do valor gerado, identificar quais resumos são mais ou menos similares uns com os outros. 
Para a Análise de Conteúdo, usamos a Biblioteca Sumy, no notebook resumos_artigos.ipynb, para criar resumos dos textos a serem analisados. Esta biblioteca foi projetada para processar textos e extrair automaticamente informações relevantes para a criação de resumos. Para pesquisas realizadas a partir de Análise de Conteúdo é possível, em primeiro lugar, reduzir a quantidade bruta de textos a serem analisadas, focando-se apenas nas sentenças mais relevantes de cada texto. Em segundo lugar, por serem classificadas pelo algoritmo como as mais relevantes, as sentenças (frases) retornadas nos resumos podem ser tomadas como as unidades de registro da análise de conteúdo. Para a Análise de Conteúdo Categorial, usamos o algoritmo Sumy para resumir os artigos analisados a um terço (33%) do tamanho original. Assim, foi possível reduzir a quantidade de texto (e o tempo de leitura para a categorização) ao mesmo tempo em que preservamos as passagens mais relevantes dos artigos. 
Nossa análise se deu investigando as relações entre as categorias, através do notebook mineracao_regras_associacao.ipynb. Usamos um algoritmo de Mineração de Dados para descobrir quais categorias ocorrem mais vezes em conjunto no nosso banco de dados. O principal objetivo destes algoritmos é encontrar padrões relevantes nos dados que possam ser usados para identificar comportamentos pouco usuais. As Regras de Associação têm como premissa básica encontrar elementos que ocorrem na presença de outros elementos em uma mesma transação, ou seja, encontrar relacionamentos ou padrões frequentes entre conjuntos de dados. Ele permite apreender quando duas ou mais categorias binárias booleanas (0 e 1, Falso e Verdadeiro) se encontram correlacionadas no conjunto de dados analisados.

REFERÊNCIAS:
Mira, B. S. de, & Castanha, R. G. (2023). Processamento de linguagem natural e acoplamento bibliográfico: uma análise da proximidade entre os artigos mais acessados do periódico Scientometrics. Informação & Informação, 27(3), 262–287. https://doi.org/10.5433/1981-8920.2022v27n3p262
